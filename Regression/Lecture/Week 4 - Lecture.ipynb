{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ridge Regression aka Lsub2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###How number of obs influence overfitting\n",
    " ####Few Observations (N small): overfit as model complexity increases\n",
    " ####Many Obs (N large): harder to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###How does # of inputs influence overfitting?\n",
    "####Many inputs\n",
    "Subject to models becoming overfit because the problem is so much more difficult. \n",
    "####Few inputs\n",
    "Easier to fit and it is unlikely that the model will become overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Balancing Fit and magnitude of coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total cost = measure of fit + magnitude of coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If measure of fit is small, then it's a good fit to the data.\n",
    "If the measure of magnitude of coefficients is small, then it is also not overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSS is the measure of fit to training data. Small RSS --> model fits training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Measure of magnitude of regression coefficients\n",
    "#####Sum over coefficients is not a good measure: because if negative you could  get a small number when even if certain numbers are very large\n",
    "#####Sum of absolute value: Is a good choice. Called Lsub1 norm\n",
    "#####Sum of squares of coefficients: also a good measure. CAlled the Lsub2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ####The Lsub2 balance formula turns out to be total cost = RSS(w) + ||w||^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Tuning parameter: λ : balance of fit and magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If λ = 0 ,  the Lsub2 goes to 0 and it is simply minimizing RSS(w) which is the old solution. This leads w-hat^least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if λ = infinity: For solutions where w-hat != 0 then total cost is infinity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If λ in between: Then ||w-hat||^2 < ||w-hat^least squares||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large λ: high bias, low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small λ: low bias, high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence λ controls your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Computing the gradient of the ridge objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||w||^2sub2 = W^T W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSS + λ||w||^2sub2   = (y-Hw)^T(y-Hw) + λw^T w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w^T w analogoous to w^2 and derivative of w^2 = 2w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Closed form solution for ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identity matrics: a matrics that has 1 going diagonal and 0 all around. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun facts: Iv = v, IA = A, A^-1A = I, AA^-1 = I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost(w) = w-hat = (H^t H + λI)^-1H^t y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complexity of inverse O(D^3)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Quiz\n",
    "1. 1\n",
    "2. high bias, low var\n",
    "3. 3\n",
    "4. stay the same, double, impossible\n",
    "5. True\n",
    "6. false\n",
    "7. 2\n",
    "8. 1, 2, 3\n",
    "9. 1, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###K- Fold Vaidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Divide data set into k groups n/k, n/k...\n",
    "#####Observations are randomly assigned into k set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with small data sets, you should use a subset of all training data for the validation set because you have a better chance of avoiding anomolies within a specific section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Bascially you train the data on  k-1 blocks and then validate on one and you interate through all the remaining blocks and take the average error of each k block. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Best value of k is 1. (k = n) also called Leave One Out Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Typically what people do is k = 5 or k = 10 and thats called 5 fold, 10 fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####This is a really important step because you never really know what n needs to be for an accurate prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##How to handle the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For the matrix notation, basically you take make hsub1 all 1. the first column of the matrix is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't really get this. lots of information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
