{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assessing Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Loss\n",
    "What do we mean: Think about how much you're losing compared with perfection. \n",
    "\n",
    "If you perfectly predict value then you have 0 loss. \n",
    "\n",
    "###Loss Function\n",
    "L(y,fsub-w-hat(x)) , basically L(y,yhat) fsubwhat = fhat(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Assessing Loss on training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training error = the average loss on houses in training set\n",
    "\n",
    "   N\n",
    "1/nΣL(y,fsub-what(x))\n",
    "  i =1\n",
    "  \n",
    "RMSE = root mean squared error = \n",
    "\n",
    "SQRT (\n",
    "   N\n",
    "1/nΣL(y,fsub-what(x))\n",
    "  n =1\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generalization error = Esubx,y[L(y,fsub-w-hat(x))]\n",
    "    E = over all possible (x,y) the weighted by how likely that value is to occur.\n",
    "    \n",
    "Unfortunately, you can't actually really compute generalization error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Test Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   N\n",
    "1/n(test)ΣL(y,fsub-what(x))\n",
    "  i =1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bascially the same as assessing loss on training, except we are using the test set instead of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A noisey approximation of our generalization error because of course we can't see every observation in the world\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overfitting if:\n",
    "1.Training error of w-hat < training error w-prime\n",
    "2. true error of w-hat > true error w-prime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Irreducible error and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 sources of error: Noise, Bias, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "symobolized as epsolon ϵ, can't really control noise and there is always some irreductible noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors arising from the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over all possible size N training sets, what does the fit look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "difference between average fit and the true function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias(x) = fsub-w(true)(x)-f-sub-w(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Philosopically, How do you know what w(true) is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "low complexity leads to high bias and high complexity leads to low bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much do specific fits vary from an expected fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "low complexity leads to low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bias variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bascially, bias and variance are opposites. As complexity goes up, bias goes down and vice-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE = bias^2 + variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with generalization error, we can't actually compute this because w(true) is not a real thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Error vs amount of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True error is going to go down as more data is added. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training error is actually going to go up as more data is added with a fixed complexity model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense because assuming complexity stays the same, a training model can easily fit those points, but as more points are added, the more difficult this becomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Training/validation/test split for model selection, fitting, assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "λ = controlling model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we choose a λ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't just choose the λmodel that minimizes test error because you're double dipping when you go to assess the performance. Essentially, you've picked the model that works best for test, but test, like training, isn't representative of generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution, create 3 data sets so you have a trainig set, validation set, and a test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you'll pick the λ that minimizes error on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then approximate generalization error using test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you'll fit your data on the training, then you'll test performance and minimize error on a validation set to select a λ and then you'll assess generalization error on a test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "typical splits are 80 training 10 valid 10 test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz\n",
    "1. false\n",
    "2. model 2\n",
    "3. impossible\n",
    "4. model 2\n",
    "5. c\n",
    "6. a\n",
    "7. false\n",
    "8. high b\n",
    "9. high v\n",
    "10. overfitted\n",
    "11. min val\n",
    "12. overly optimistic (Wrong) 2. over opt, avoids overfitting 3. tried all three, still wrong\n",
    "13. var goes to 0 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
